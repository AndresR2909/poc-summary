{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "# Step 1: Download model checkpoints. \n",
    "\n",
    "If you want to further finetune the instruction variant of Llama Llama 3.2 3B, you can download it via the following command:\n",
    "\n",
    "```bash\n",
    "litgpt download meta-llama/Llama-3.2-3B-Instruct --access_token hf_...\n",
    "```\n",
    "\n",
    "(Note that some models, such as Llama 3.2, require that you accept Meta AI's terms of service for this model, and you need to use a special access token via the `--access_token ...` option. For more information, visit the respective Model Hub website, e.g., [meta-llama/Llama-3.2-3B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-3B). The access token can be created under your Model Hub in the `Profile > Access Tokens` menu.)\n",
    "\n",
    "(To list the other available models, execute `litgpt download list` .)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;  \n",
    "# **Step 2: Finetune the Model**  \n",
    "\n",
    "This section illustrates how to fine-tune the model on a custom dataset. In this case, the **Supervised Fine-Tuning (SFT) dataset** is located in the folder **`data/train.csv`**.  \n",
    "\n",
    "### **Data Preparation:**  \n",
    "1. **Load the dataset** from `data/train.csv`.  \n",
    "2. **Split it into training and validation sets** to ensure proper evaluation.  \n",
    "3. **Convert the dataset into the lit-GPT format**, following this structure:  \n",
    "\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"instruction\": \"How does the refund process work for canceled orders?\",\n",
    "    \"input\": \"\",\n",
    "    \"output\": \"If you cancel an order, the refund process depends on the payment method used. Typically, refunds are processed within 5-7 business days. Please check your bank statement for confirmation.\"\n",
    "  },\n",
    "  {\n",
    "    \"instruction\": \"What payment methods do you accept?\",\n",
    "    \"input\": \"\",\n",
    "    \"output\": \"We accept credit cards, PayPal, and Apple Pay. Please visit our payments page for more details.\"\n",
    "  },\n",
    "  ...\n",
    "]\n",
    "```\n",
    "\n",
    "To finetune the model, we use the following command: \n",
    "\n",
    "```bash\n",
    "litgpt finetune_lora meta-llama/Llama-3.2-1B-Instruct \\\n",
    "  --data JSON \\\n",
    "  --data.json_path my_custom_dataset.json \\\n",
    "  --data.val_split_fraction 0.1 \\\n",
    "  --train.epochs 1 \\\n",
    "  --out_dir out/llama-3.2-3b-finetuned \\\n",
    "  --precision bf16-true\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer\n",
    "import wandb\n",
    "import torch\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/Lightning-AI/litgpt/blob/main/tutorials/finetune_full.md#tune-on-your-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset_name = \"AndresR2909/youtube_transcriptions_summaries_2025_gpt4.1\"\n",
    "splits = {'train': 'data/train-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}\n",
    "df_test = pd.read_parquet(f\"hf://datasets/{dataset_name}/\" + splits[\"test\"])\n",
    "df_train = pd.read_parquet(f\"hf://datasets/{dataset_name}/\" + splits[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 221 entries, 0 to 220\n",
      "Data columns (total 15 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   channel_name       221 non-null    object \n",
      " 1   video_id           221 non-null    object \n",
      " 2   source             221 non-null    object \n",
      " 3   publish_date       158 non-null    object \n",
      " 4   duration           221 non-null    float64\n",
      " 5   last_update_date   221 non-null    object \n",
      " 6   title              221 non-null    object \n",
      " 7   text               221 non-null    object \n",
      " 8   year               221 non-null    int64  \n",
      " 9   month              158 non-null    float64\n",
      " 10  number_of_tokenks  221 non-null    int64  \n",
      " 11  prompt             221 non-null    object \n",
      " 12  summary            221 non-null    object \n",
      " 13  key_terms          221 non-null    object \n",
      " 14  __index_level_0__  221 non-null    int64  \n",
      "dtypes: float64(2), int64(3), object(10)\n",
      "memory usage: 26.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2004 entries, 0 to 2003\n",
      "Data columns (total 15 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   channel_name       2004 non-null   object \n",
      " 1   video_id           2004 non-null   object \n",
      " 2   source             2004 non-null   object \n",
      " 3   publish_date       1452 non-null   object \n",
      " 4   duration           2004 non-null   float64\n",
      " 5   last_update_date   2004 non-null   object \n",
      " 6   title              2004 non-null   object \n",
      " 7   text               2004 non-null   object \n",
      " 8   year               2004 non-null   int64  \n",
      " 9   month              1452 non-null   float64\n",
      " 10  number_of_tokenks  2004 non-null   int64  \n",
      " 11  prompt             2004 non-null   object \n",
      " 12  summary            2004 non-null   object \n",
      " 13  key_terms          2004 non-null   object \n",
      " 14  __index_level_0__  2004 non-null   int64  \n",
      "dtypes: float64(2), int64(3), object(10)\n",
      "memory usage: 235.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('checkpoints/meta-llama/Llama-3.2-3B-Instruct')\n",
    "df_train[\"llama_tokens\"] = df_train[\"text\"].apply(lambda x: len(tokenizer.encode(str(x))))\n",
    "df_mas_largos = df_train[df_train[\"llama_tokens\"] >= 4096] #8192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_train = df_train[df_train[\"llama_tokens\"] < 4096]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>llama_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1021.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2493.776690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1053.139173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>42.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1935.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2702.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3324.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4084.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       llama_tokens\n",
       "count   1021.000000\n",
       "mean    2493.776690\n",
       "std     1053.139173\n",
       "min       42.000000\n",
       "25%     1935.000000\n",
       "50%     2702.000000\n",
       "75%     3324.000000\n",
       "max     4084.000000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df_train[['llama_tokens']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_name</th>\n",
       "      <th>llama_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>983</td>\n",
       "      <td>983.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>USACRYPTONOTICIAS</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>688</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>14273.471007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>7589.613838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4096.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>6055.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>14203.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>21249.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>48608.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             channel_name  llama_tokens\n",
       "count                 983    983.000000\n",
       "unique                  6           NaN\n",
       "top     USACRYPTONOTICIAS           NaN\n",
       "freq                  688           NaN\n",
       "mean                  NaN  14273.471007\n",
       "std                   NaN   7589.613838\n",
       "min                   NaN   4096.000000\n",
       "25%                   NaN   6055.500000\n",
       "50%                   NaN  14203.000000\n",
       "75%                   NaN  21249.500000\n",
       "max                   NaN  48608.000000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mas_largos[['channel_name','llama_tokens']].describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actúa como un experto en trading y análisis de mercados financieros.\n",
      "\n",
      "INSTRUCCIONES:\n",
      "1. Analiza el texto proporcionado entre las líneas de guiones.\n",
      "2. Elabora un informe estructurado siguiendo exactamente el formato solicitado.\n",
      "3. Utiliza un lenguaje claro, conciso y relevante para inversores.\n",
      "4. No inventes información; limita tu análisis únicamente al contenido del texto.\n",
      "\n",
      "FORMATO DEL INFORME:\n",
      "- **Introducción:** Presenta una visión general del tema tratado.\n",
      "- **Puntos clave:** Resume los aspectos más importantes en formato de viñetas.\n",
      "- **Conclusión:** Ofrece un cierre que sintetice el análisis realizado.\n",
      "- **Activos recomendados:** Extrae y lista, en una sección aparte, todos los activos mencionados como opciones de inversión.\n",
      "\n",
      "Texto a analizar:\n",
      "------------\n",
      "{context}\n",
      "------------\n",
      "\n",
      "Recuerda: Sigue el formato solicitado y asegúrate de que la información sea precisa y útil para inversores.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Leer la instrucción desde el archivo\n",
    "with open('prompts/v3_summary_expert.txt', 'r', encoding='utf-8') as f:\n",
    "    instruction_template = f.read()\n",
    "\n",
    "print(instruction_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos guardados: data/train_data.json y data/test_data.json\n"
     ]
    }
   ],
   "source": [
    "def convert_to_json_format(df,instruction):\n",
    "    return [\n",
    "        {\n",
    "            \"instruction\": instruction,\n",
    "            \"input\": row[\"text\"],\n",
    "            \"output\": row[\"summary\"]\n",
    "        }\n",
    "        for _, row in df.iterrows()\n",
    "    ]\n",
    "\n",
    "# Convertir train y test\n",
    "train_data_litgpt = convert_to_json_format(new_df_train,instruction_template)\n",
    "test_data_litgpt = convert_to_json_format(df_test,instruction_template)\n",
    "\n",
    "# Guardar a archivos JSON\n",
    "with open('data/train_data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(train_data_litgpt, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open('data/test_data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(test_data_litgpt, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Archivos guardados: data/train_data.json y data/test_data.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "\n",
    "litgpt finetune_lora meta-llama/Llama-3.2-3B-Instruct \\\n",
    "  --devices 1 \\\n",
    "  --data JSON \\\n",
    "  --data.json_path data/train_data.json \\\n",
    "  --data.val_split_fraction 0.1 \\\n",
    "  --train.epochs 1 \\\n",
    "  --train.max_seq_length 4096 \\\n",
    "  --train.global_batch_size 2 \\\n",
    "  --eval.max_new_tokens 800 \\\n",
    "  --out_dir out/llama-3.2-3b-finetuned_bnb_int8 \\\n",
    "  --logger_name wandb \\\n",
    "  --precision bf16-true \\\n",
    "  --quantize bnb.nf4\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;  \n",
    "# **Step 3: Deploy the Model**  \n",
    "\n",
    "This section explains how to deploy the fine-tuned model and use it to generate responses for the **questions in `data/test.csv`**. We will set up an inference server using [LitServe](https://github.com/Lightning-AI/LitServe), a high-performance serving tool integrated into **lit-GPT**.\n",
    "\n",
    "\n",
    "## **3.1: Query the Inference Server with `df_test data`**  \n",
    "\n",
    "To launch an inference server that serves the fine-tuned model (e.g., **Llama 3.2 1B** stored in `checkpoints/meta-llama/Llama-3.2-1B`), use the following command:\n",
    "\n",
    "```bash\n",
    "litgpt serve out/llama-3.2-3b-finetuned_bnb_nf4_v2/final --max_new_tokens 1200 --temperature 0.0 --top_p 0.9\n",
    "litgpt serve out/llama-3.2-3b-finetuned_bnb_nf4/final --max_new_tokens 1200 --temperature 0.0 --top_p 0.9\n",
    "litgpt serve out/llama-3.2-3b-finetuned_v1/final --max_new_tokens 1200 --temperature 0.0 --top_p 0.9\n",
    "litgpt serve out/llama-3.2-1b-finetuned_v2/final --max_new_tokens 1200 --temperature 0.0 --top_p 0.9\n",
    "litgpt serve out/llama-3.2-1b-finetuned_v5/final --max_new_tokens 1200 --temperature 0.0 --top_p 0.9\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('checkpoints/meta-llama/Llama-3.2-3B-Instruct')\n",
    "df_test[\"llama_tokens\"] = df_test[\"text\"].apply(lambda x: len(tokenizer.encode(str(x))))\n",
    "new_df_test = df_test[df_test[\"llama_tokens\"] < 8192]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>llama_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>221.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7398.990950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7619.794179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>39.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2379.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3658.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>11808.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>31448.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       llama_tokens\n",
       "count    221.000000\n",
       "mean    7398.990950\n",
       "std     7619.794179\n",
       "min       39.000000\n",
       "25%     2379.000000\n",
       "50%     3658.000000\n",
       "75%    11808.000000\n",
       "max    31448.000000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[['llama_tokens']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>llama_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>158.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3014.689873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1506.119699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>39.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2042.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3015.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3879.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7979.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       llama_tokens\n",
       "count    158.000000\n",
       "mean    3014.689873\n",
       "std     1506.119699\n",
       "min       39.000000\n",
       "25%     2042.500000\n",
       "50%     3015.000000\n",
       "75%     3879.250000\n",
       "max     7979.000000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df_test[['llama_tokens']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 220\n",
      "Model Response: - **Introducción:**  \n",
      "El texto analiza el momento en el que los inversores deberían entrar en activos, con especial en bitcoin (BTC), considerando el comportamiento de los mercados y los patrones técnicos que podría llevar al precio. Se hace referencia a técnicas de trading y se mencionan movimientos alcistas en el gráfico semanal y diario.\n",
      "\n",
      "- **Puntos clave:**\n",
      "  - Se recomienda buscar entrada en activos como bitcoin, especialmente en el momento de la onda B, considerando posibles movimientos alcistas a corto plazo.\n",
      "  - Se sugiere priorizar las entradas por cierre de la onda A en el gráfico semanal y posteriormente buscar oportunidades de long en el gráfico semanal, preferiblemente después de una onda B.\n",
      "  - Es probable que el precio de BTC sea cerca de los 95,000 (un 40% de ganancia sobre los 25,000 dólares), pero se espera esperar a que el precio se acerque a la parte baja de la onda B antes de realizar la entrada.\n",
      "  - Se menciona que el mercado parece manipulado por el apoyo de un \"man\" (presumiblemente John McAfee), quien puede estar moviendo los mercados a sus propios ritmos.\n",
      "  - Se sugiere buscar centavos y \"gripas\" en el proceso de entrada, evitando la pérdida de toda la plata.\n",
      "  - Se recomienda la práctica de \"cuerpos\" o \"trajes\" de trading, que pueden ser una forma eficiente de absorber los desplazamientos de precio y buscar oportunidades de negociación.\n",
      "  - Se advierte que el proceso de entrada puede requerir paciencia y una estrategia bien definida, con enfoque en el momento de la onda B y el seguimiento del patrón técnico.\n",
      "\n",
      "- **Conclusión:**  \n",
      "El análisis sugiere que buscar entrada en activos como bitcoin durante la onda B puede ser una estrategia defensiva para evitar los impulsores técnicos y aprovechar posibles movimientos alcistas que se están desarrollando en el gráfico semanal. La paciencia y la planificación son fundamentales para acceder a oportunidades de inversión y aprovechar al máximo los ganancias.\n",
      "\n",
      "- **Activos recomendados:**  \n",
      "  - Bitcoin (BTC)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Cargar el dataset de prueba\n",
    "test_data = new_df_test.copy()\n",
    "\n",
    "# Lista para almacenar las respuestas generadas\n",
    "results = []\n",
    "\n",
    "\n",
    "# Iterar sobre cada instrucción y consultar el modelo\n",
    "for index, row in test_data.iterrows():\n",
    "    channel_name = row[\"channel_name\"]\n",
    "    video_id = row[\"video_id\"]\n",
    "    input = row[\"text\"]\n",
    "    query = row[\"prompt\"]\n",
    "    reference = row[\"summary\"]\n",
    "    \n",
    "    try:\n",
    "        # Realizar una solicitud POST al modelo\n",
    "        response = requests.post(\n",
    "            \"http://127.0.0.1:8000/predict\",\n",
    "            json={\"prompt\": query}\n",
    "        )\n",
    "        \n",
    "        # Obtener el texto de la respuesta del modelo\n",
    "        generated_response = response.json().get('output', '')\n",
    "        \n",
    "        # Limpiar la salida anterior\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        # Imprimir  respuesta\n",
    "        print(f\"index: {index}\")\n",
    "        print(f\"Model Response: {generated_response}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"error: {e}\")\n",
    "        generated_response = None\n",
    "    \n",
    "    # Agregar el resultado a la lista\n",
    "    results.append({\n",
    "        \"channel_name\":channel_name,\n",
    "        \"video_id\":video_id,\n",
    "        \"input\":input,\n",
    "        \"instruction\": instruction_template,\n",
    "        \"prompt\":query,\n",
    "        \"generated_response\": generated_response,\n",
    "        \"reference\":reference\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuestas guardadas en data\n"
     ]
    }
   ],
   "source": [
    "# Crear un DataFrame con los resultados\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Guardar los resultados en un archivo CSV\n",
    "results_df.to_csv(f\"data/llama-3.2-1b-finetuned_v5.csv\", index=False, sep=\";\")\n",
    "\n",
    "print(\"Respuestas guardadas en data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 4:  Merge LoRA weights:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "litgpt merge_lora out/llama-3.2-3b-finetuned_bnb_nf4_v2/final\n",
    "litgpt merge_lora out/llama-3.2-1b-finetuned_v5/final\n",
    "litgpt merge_lora out/llama-3.2-3b-finetuned_bnb_nf4/final\n",
    "litgpt merge_lora out/llama-3.2-3b-finetuned_v1/final\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 5: Convert the finetuning model back into a HF format:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "litgpt convert_from_litgpt out/llama-3.2-3b-finetuned_bnb_nf4_v2/final/ out/hf-llama-3.2-3b-finetuned_bnb_nf4_v2/converted/\n",
    "\n",
    "litgpt convert_from_litgpt out/llama-3.2-3b-finetuned_bnb_nf4/final/ out/llama-3.2-3b-finetuned_bnb_nf4/converted/\n",
    "\n",
    "litgpt convert_from_litgpt out/llama-3.2-3b-finetuned_v1/final/ out/llama-3.2-3b-finetuned_v1/converted/\n",
    "\n",
    "litgpt convert_from_litgpt out/llama-3.2-1b-finetuned_v5/final/ out/hf-llama-3.2-1b-finetuned_v5/converted/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 6: Instence hf model and push it to hf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b42bf9c8d434befa68b3d595de2ab85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e72fdaf921d34c0593e6e6e241f7e34b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7734de5400e94a62ad4706bc2ba1381d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2. Crea el modelo\n",
    "model = AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-1B-Instruct')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-3.2-1B-Instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer.json: 100%|██████████| 17.2M/17.2M [00:45<00:00, 375kB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/AndresR2909/hf-llama-3.2-1b-finetuned_v5/commit/a980d802b28228454740436a221f8c3d6f881705', commit_message='Upload tokenizer', commit_description='', oid='a980d802b28228454740436a221f8c3d6f881705', pr_url=None, repo_url=RepoUrl('https://huggingface.co/AndresR2909/hf-llama-3.2-1b-finetuned_v5', endpoint='https://huggingface.co', repo_type='model', repo_id='AndresR2909/hf-llama-3.2-1b-finetuned_v5'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(\"AndresR2909/hf-llama-3.2-1b-finetuned_v5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Carga tus pesos\n",
    "state_dict = torch.load('out/hf-llama-3.2-1b-finetuned_v5/converted/model.pth')\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "872e4b1b1804401f8df455af9092d3a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a0cefeffc184ad393ebd450e5bad1c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fc2e1854de840d49071db2eb59edf91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/AndresR2909/hf-llama-3.2-1b-finetuned_v5/commit/90693e936700b9eff030ee685c5b7de95deab128', commit_message='Upload LlamaForCausalLM', commit_description='', oid='90693e936700b9eff030ee685c5b7de95deab128', pr_url=None, repo_url=RepoUrl('https://huggingface.co/AndresR2909/hf-llama-3.2-1b-finetuned_v5', endpoint='https://huggingface.co', repo_type='model', repo_id='AndresR2909/hf-llama-3.2-1b-finetuned_v5'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subes al Hub:\n",
    "model.push_to_hub(\"AndresR2909/hf-llama-3.2-1b-finetuned_v5\")\n",
    "tokenizer.push_to_hub(\"AndresR2909/hf-llama-3.2-1b-finetuned_v5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
